#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass IEEEtran
\begin_preamble
\usepackage{algpseudocode}
\end_preamble
\options journal
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding default
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\float_placement tbh
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_title "Your Title"
\pdf_author "Your Name"
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle false
\pdf_quoted_options "pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false"
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Edges Based Surgical Instrument Tracking in Laparoscopic Images
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
preliminar
\end_layout

\end_inset


\end_layout

\begin_layout Author
G.
 M.
 Miranda, P.
 H.
 S.
 Perruci, M.
 C.
 Bernardes and R.
 F.
 Vergara
\end_layout

\begin_layout Abstract
The replacement of arduous tasks by machines has been growing in last years,
 promoting practicality in different areas.
 Robotics in medicine sets one of these contexts, particularly in minimally
 invasive surgeries, the scope of our work.
 In this paper, we propose a method for tracking surgical instruments in
 laparoscopy, shipped in a robotic insurer and used in aid for surgeons
 in SUS, the Brazilian health system.
 The approach is based on edges in a totally 2D environment without any
 notion of patient's configuration.
 The algorithm was validated with frames from our dataset, achieving a mean
 error 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
mu$
\end_layout

\end_inset

 of 15,53 pixels for high-quality images and 17,82 pixels for low-quality
 according to our metrics.
 For benchmarking, we considered the mean fps 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
eta$
\end_layout

\end_inset

 , that in our system reached a value of 24,62 fps.
 The detection proved valid considering videos gathered from internet and
 simulated in laboratory.
\end_layout

\begin_layout Keywords
Instrument tracking, Laparoscopic surgery, Vision- based tracking, Edges-
 based Tool
\end_layout

\begin_layout Peer Review Title
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Don't add text here!
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
required if you use the document class option 
\family sans
peerreview
\family default
, must otherwise not be used
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Laparoscopic surgery is one of most performed image guided minimally invasive
 procedures.
 Generally, it is performed by two surgeons: the main physician conducts
 the procedure by the means of long and thin instruments inserted on the
 patients abdominal cavity, while an assisting surgeon is needed for holding
 the endoscope lenses inside the patient during the entire procedure.
 For both of them, Operation Room (OR)'s fatigue has been reported as a
 serious ergonomics concern 
\begin_inset CommandInset citation
LatexCommand cite
key "article:supe:2010"

\end_inset

.
\end_layout

\begin_layout Standard
Medical robotic endoscope holders have been developed to replace the assistant
 surgeon in the OR, leaving the camera's control to the main surgeon.
 Systems such Aesop® 
\begin_inset CommandInset citation
LatexCommand cite
key "ballester2001comparison"

\end_inset

 and Vicky® 
\begin_inset CommandInset citation
LatexCommand cite
key "long2007development"

\end_inset

 endoscope holders offer the surgeon full control over the visualization
 of the surgical operation and have an increasing interest of the medical
 society.
 However, due to the high costs of importation of any of these products
 for use in Brazilian's health system (SUS), University of Brasilia's project
 CLARA aims to develop a national alternative for the technology.
\end_layout

\begin_layout Standard
Even though robotic endoscope holders have been designed to receive commands
 by ergonomically comfortable means, the surgeon still needs to deviate
 his attention of the procedure to move the device, which might compromise
 the procedure.
 The present work proposes a new method for visual tracking of surgical
 instruments that could be used to automatically command CLARA's robotic
 endoscope holder.
 Our main intention on tracking the tip of the laparoscopic instrument is
 to autonomously orient the movement of the endoscope holder in order to
 center the images at the surgical tool whenever requested by the surgeon.
 
\end_layout

\begin_layout Standard
The related methods from the state of the art report some variety of approaches.
 The most simple and effective approach is tracking a colored marker, added
 to the tip of the instrument 
\begin_inset CommandInset citation
LatexCommand cite
key "wei1997real"

\end_inset

.
 Some strategies were even able to acquire 3D positioning from an appropriate
 set of markers 
\begin_inset CommandInset citation
LatexCommand cite
key "bouarfa2012vivo"

\end_inset

.
 Although their results seems quite promising, the need of a biocompatible
 marker restrains the solution to only custom made marked instruments.
 A wider range approach is to train a classifier, similar to a face tracking,
 applied to surgical instruments 
\begin_inset CommandInset citation
LatexCommand cite
key "sznitman2012data"

\end_inset

.
 It has been reported near real time computer efficiency 
\begin_inset CommandInset citation
LatexCommand cite
key "allan20142d"

\end_inset

, although the method involves intensive training of carefully chosen datasets.
 The most generic and direct approach found in the literature is to track
 the surgical instruments by the edges found on image .
 Though it involves high computational cost processing, this solution is
 able to extract the instrument's position with considerable precision.
 More ambitious approaches use the insertion point's position of each instrument
 to filter edges and estimate the 3D position of the tip near real time
 
\begin_inset CommandInset citation
LatexCommand cite
key "Voros2007"

\end_inset

.
\end_layout

\begin_layout Standard
This paper presents a novel visual tracking method for surgical instruments.
 It comprises an edges based approach capable of performing 2D pose estimation
 of the instrument's tip, built upon geometrical constraints observed on
 the instruments.
 The main contribution of this research is to develop a low cost surgical
 instrument tracker, robust to occlusion and color changes.
 The method requires no prior knowledge about the surgical environment and
 the only restraint is the need of appropriate contrast between the instrument's
 axis and the background of the video frame.
 Although the algorithm was designed to be embedded on CLARA's robotic endoscope
 holder, the solution is expected to develop properly in different types
 of minimally invasive operations.
 Tests will be performed on videos of both 
\emph on
in vivo 
\emph default
real surgical procedures and 
\emph on
ex vivo
\emph default
 simulations prepared in laboratory.
\end_layout

\begin_layout Standard
On section 2, we describe our method in extraction the tip's position in
 surgical instruments.
 Section 3 shows the results obtained by us, which is measured by a distance
 approuch of obtained and ideal positions.
 It also is evaluated for benchmark purpouses.
 In section 4 we take a brief discussion about our work and some problems
 encountered by us.
 The last section concludes the paper and describes future work.
 
\end_layout

\begin_layout Section
Materials and Methods
\end_layout

\begin_layout Standard
The proposed approach for tracking surgical instruments in laparoscopic
 surgery videos is based on the recognition of the tool geometric characteristic
s on the image.
 The tool shaft can be represented in the image by two lines, almost parallel
 and close to each other.
 These conditions are sufficient to filter probable lines from those present
 in the image, locate the extremity of the shaft and estimate the instrument
 tip position.
 Known geometrical constraints concerning the tool's shaft have been reported
 by 
\begin_inset CommandInset citation
LatexCommand cite
key "Voros2007"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "DockterII2013"

\end_inset

 and have shown precise and robust results for stereo camera images.
 
\end_layout

\begin_layout Standard
In this paper we propose a method based on these previous studies, but adapted
 to monocular endoscope captures with no prior knowledge about the surgical
 environment.
 These are the conditions commonly found on SUS.
 As a consequence, we expect to develop a robust method to estimate the
 2D pose of a single instrument tip.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Our implementation can be summarized in four stages: preprocessing, edges
 extraction, lines extraction tools shaft extraction and locate instrument's
 tip.
 The processes's flow is represented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:generalFlux"

\end_inset

, a general fluxogram which leads from the laparoscopic video fram to the
 estimation of instrument's tip position.
 Each module will be briefly explained on the following subsections.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our implementation can be summarized in five stages: preprocessing, edges
 extraction, lines extraction, tools shaft extraction and locate instrument's
 tip.
 The processes's flow is represented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:generalFlux"

\end_inset

, a general fluxogram which leads from the laparoscopic video fram to the
 estimation of instrument's tip position.
 Each module will be briefly explained on the following subsections.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement t
wide true
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/fluxograma.png
	lyxscale 10
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Representation of our methodology in a general flowchart.
 As an input, the laparoscopic video frame goes through preprocessing, to
 prepare it for line's extraction.
 Then, the vector of lines is examined for possible tools shaft and, then,
 possible tool's tip.
 Located tip's positions are returned as output of the implementation.
\begin_inset CommandInset label
LatexCommand label
name "fig:generalFlux"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Preprocessing
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The first part of our implementation consists on detaching the surgical
 tools from the image's background.
 Also, luminous variations as shadows must be smothened.
 Therefore, it is performed a histogram equalization of each of the image
 channels.
 Equalized histogram smooths the general contrast on the image, removing
 false edges caused by shadows.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The first part of our implementation consists on detaching the surgical
 tools from the image's background.
 Also, luminous variations as shadows must be smothened.
 Therefore, it is performed a histogram equalization on each of the image
 channels.
 Equalized histogram increases the global contrast of the image, allowing
 a better vizualization of the instrument.
\end_layout

\begin_layout Itemize
Edges Extraction
\end_layout

\begin_layout Standard
Then, a color threshold is applied, clustering similar characteristics of
 the environment in order to differ the black surgical tools from the red
 background on the patient's body.
 In the sequence, a Canny edges detection algorithm is used.
 It consists in extracting image's gradients and select best edge points
 candidates by applying a double threshold on their gradient values.
 To avoid the effect of poorly illuminated regions in the corners of the
 frame, we consider an ellipsoid Region Of Interest (ROI) that ignores those
 darker regions.
\end_layout

\begin_layout Itemize
Lines Extraction
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
On the second phase, we focus on searching for the two lines that best fit
 the instrument shape.
 Canny Edges Detection 
\begin_inset CommandInset citation
LatexCommand cite
key "Canny"

\end_inset

 acts on the resulting image of the preprocessing phase.
 Edges detection methods consists in extracting image's derivatives in all
 directions, or gradients.
 High values for gradients implies distinctive edges observed on image.
 Canny's specific method selects the best edges points candidates by applying
 a double threshold on their gradient's values.
 The expected output is illustrated as a binary image of edges, as Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Extraction-from-background"

\end_inset


\begin_inset Formula $(d)$
\end_inset

.
\end_layout

\begin_layout Plain Layout
Upon the edges image, Hough Transform 
\begin_inset CommandInset citation
LatexCommand cite
key "Hough"

\end_inset

 is called to extract lines formed by them.
 It consists in a voting operation in which every possible lines passing
 through each pair of edge points is computed and stored in the 
\emph on
Hough Space
\emph default
 --- a cartesian plan, in function of line's polar coordinates (
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
rho$, $
\backslash
theta$
\end_layout

\end_inset

).
 As a result it's obtained a vector of line's coordinates, ordained by their
 votes.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
On the second phase, we focus on searching for the two lines that best fit
 the instrument's shape.
 Upon the edges image, Hough Transform 
\begin_inset CommandInset citation
LatexCommand cite
key "Hough"

\end_inset

 is called to extract lines formed by them.
 It consists in a voting operation in which every possible lines passing
 through each pair of edge points is computed and stored in the 
\emph on
Hough Space
\emph default
 --- a cartesian plan, in function of line's polar coordinates (
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
rho$, $
\backslash
theta$
\end_layout

\end_inset

).
 As a result it's obtained a vector of line's coordinates, ordained by their
 votes.
\end_layout

\begin_layout Itemize
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Instrument Body Extraction
\end_layout

\end_inset

 Tool Shaft Extraction
\end_layout

\begin_layout Standard
In order to identify the instrument's body among all lines found on image,
 an outer loop was designed to filter the lines upon their relevance and
 the expected geometrical constraints.
 Because Hough Lines already outputs lines ordered by votes, it is assumed
 that the first lines of the vector belongs to the instrument's axis.
 So, an ineer loop iterates through the vector searching for its pair.
 The second line must be close to the first line in both their polar parameters
 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
rho$
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
theta$
\end_layout

\end_inset

, meaning that 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
theta min < 
\backslash
Delta
\backslash
theta < 
\backslash
theta max$
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
rho min < 
\backslash
Delta
\backslash
rho < 
\backslash
rho max$
\end_layout

\end_inset

 .
 When a valid pair is found, the inner loop --- for the second line ---
 is satisfied and an instrument's body is found.
 Then, the outer loop goes on, searching for instrument's lines which fulfill
 the geometrical restraints until the vector of lines is done.
 By the end of this phase, it's obtained a vector of possible instrument's
 bodies on the image.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Geometrical constraints for lines in instrument's body.
\begin_inset CommandInset label
LatexCommand label
name "tb::body_constraints"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Angle
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
theta min < 
\backslash
Delta
\backslash
theta < 
\backslash
theta max$
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Module
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
rho min < 
\backslash
Delta
\backslash
rho < 
\backslash
rho max$
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Locate Instrument's Tip
\end_layout

\end_inset

 Tool Tip Location
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The primary objective of this phase is to locate the end of the tool's shaft
 in each of the possible instrument's bodies.
 A loop goes through the vector of bodies, analyzing each pair of lines.
 It's assumed that both of the axis ending points are represented as edges
 returned by Canny.
 So, a possible ending point is every point of the edges image which belongs
 to the line.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:findTip"

\end_inset


\begin_inset Formula $(c)$
\end_inset

, candidates for ending points found are represented on the center of the
 circles.
 
\end_layout

\begin_layout Plain Layout
In order to locate the actual, or the best, ending point among the circled
 of figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:findTip"

\end_inset


\begin_inset Formula $(c)$
\end_inset

, a restraint is needed.
 Observe that an ideal ending point 
\begin_inset ERT
status open

\begin_layout Plain Layout

$Pi$
\end_layout

\end_inset

 has neighbor points at one side, though none in the other.
 As shown in Figure [Z], there are edge points along the axis, but none
 after 
\begin_inset Formula $Pi$
\end_inset

.
 So, it's assumed that a possible ending point 
\begin_inset ERT
status open

\begin_layout Plain Layout

$Po$
\end_layout

\end_inset

 can only be an ending point if the restraints expressed in table [Y] are
 fulfilled.
 The parameter 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma$
\end_layout

\end_inset

 is a point's tolerance margin for the computation.
 If the point 
\begin_inset Formula $Po$
\end_inset

 can be expressed as 
\begin_inset Formula $Po=Po.x+Po.y$
\end_inset

 , then 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma$
\end_layout

\end_inset

 is 
\begin_inset Formula $\gamma=\gamma.x+\gamma.y$
\end_inset

 , both vectors of 
\begin_inset ERT
status open

\begin_layout Plain Layout

$R^{2}$
\end_layout

\end_inset

.
 In figure [Z], the blue filled circles represent the best ending points
 candidates for the frame.
\end_layout

\begin_layout Plain Layout
Still, the instrument's tip is located in the middle the shaft's ending
 points 
\begin_inset Formula $Pi',Pi''$
\end_inset

, from different body's lines.
 So, in order to estimate the tip's location through the filtered ending
 points, we need to correspond 
\begin_inset Formula $Po^{'}$
\end_inset

 to its pair, 
\begin_inset Formula $Po^{''}$
\end_inset

, on the other line.
 These points must be positioned close to each other, respecting the restraints
 expressed in table[Y].
 When a successful correspondence occurs, the instrument's tip pose estimation
 is expressed as the arithmetic mean of the corresponded points, as expressed
 in equation [E].
 For multiple correspondences, equation [E] expresses the tip's pose estimation.
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
Tip=\frac{SPo_{1}+SPo_{2}}{2}
\]

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "eq::correspond-1"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
Tip=\frac{SPo_{1}+SPo_{2}+\ldots+SP_{n}}{n}
\]

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Constraints for correspondence.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Neighbors
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma - Pt < Pt < 
\backslash
gamma + Pt$
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Correspondence
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma - Pt < 
\backslash
Delta Pt < Pt + 
\backslash
gamma$
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/FindTip/final.jpg
	width 45col%
	height 35col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/FindTip/shaft_lines.jpg
	width 45col%
	height 35col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/FindTip/bestPoints.jpg
	width 45col%
	height 35col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/FindTip/lines_bestPoints.jpg
	width 45col%
	height 35col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Visual representation of 
\emph on
Locate Intrument's Tip process
\emph default
.

\emph on
 
\emph default
Subfigure 
\begin_inset Formula $(a)$
\end_inset

 shows, in green, the resulting pose estimation for the tip on the original
 image.
 Subfigure 
\begin_inset Formula $(b)$
\end_inset

, shows the tools shaft --- extracted on 
\emph on
Intrument's Body Extraction
\emph default
 phase --- drawn upon the binary image of 
\emph on
Canny
\emph default
 edges.
 Each of the edge points which belongs to the tools shaft lines is located
 by collored circles in Subfigure 
\begin_inset Formula $(c)$
\end_inset

.
 These are the candidates for ending point found.
 Yellow circles locate the points wich fullfill the geometrical contraints
 of Table 2.
 Finally, Subfigure 
\begin_inset Formula $(d)$
\end_inset

 express in yellow lines the correspondence of candidates between both the
 shaft lines.
 Observe that two correspondences of yellow points fullfilled the 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma$
\end_layout

\end_inset

 constraints of equation [Z].
 Finally the instrument's tip pose estimation is represented in green, located
 at the middle of the yellow lines.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:findTip"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The primary objective of this phase is to locate the end of the tool's shaft
 in each of the instrument's bodies.
 A loop goes through the vector of bodies, analyzing each pair of lines.
 It's assumed that both of the axis ending points are represented as edges
 returned by Edges Extraction step.
 So, a possible ending point is every point of the edges image which belongs
 to the line.
 A step for finding ending point candidates is performed.
 In order to locate the actual, or the best, ending point 
\begin_inset ERT
status open

\begin_layout Plain Layout

$P0$
\end_layout

\end_inset

, it has to have neighbor points at one side, though none in the other.
 A tolerance 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma$
\end_layout

\end_inset

 for this computation is used.
 If the point 
\begin_inset Formula $Po$
\end_inset

 can be expressed as 
\begin_inset Formula $Po=Po.x+Po.y$
\end_inset

 , then 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma$
\end_layout

\end_inset

 is 
\begin_inset Formula $\gamma=\gamma.x+\gamma.y$
\end_inset

 .
 In order to estimate the tip's location through both body's lines, we need
 to correspond 
\begin_inset Formula $Po^{'}$
\end_inset

 to its pair, 
\begin_inset Formula $Po^{''}$
\end_inset

.
 These points must be positioned close to each other, respecting a second
 restraint, given by 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
gamma - Pt < 
\backslash
Delta Pt < Pt + 
\backslash
gamma$
\end_layout

\end_inset

.
 When a successful correspondence occurs, the instrument's tip pose estimation
 is expressed as the arithmetic mean of the corresponded points.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout

\size footnotesize
\begin_inset Caption Standard

\begin_layout Plain Layout
Pseudocode for surgical tool detection
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\size footnotesize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\begin_layout Plain Layout


\backslash
Procedure{Locate Tools Tip}{}
\end_layout

\begin_layout Plain Layout

	
\backslash
State Begin
\end_layout

\begin_layout Plain Layout

		
\backslash
State $ 
\backslash
textit{image} 
\backslash
gets videoFrame $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		
\backslash
State $ 
\backslash
textit{image} 
\backslash
gets 
\backslash
emph{PREPROCESSING}(
\backslash
textit{image}) $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		
\backslash
State $ 
\backslash
textit{image} 
\backslash
gets 
\backslash
emph{EDGESEXTRACTION}(
\backslash
textit{image}) $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		
\backslash
State $ 
\backslash
textit{vector of lines} 
\backslash
gets 
\backslash
emph{EXTRACTLINES}(
\backslash
textit{image}) $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		
\backslash
Loop { each line in vector of lines }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

			
\backslash
State $ 
\backslash
textit{ first line } 
\backslash
gets 
\backslash
emph{Most Relevant Line(line)} $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

			
\backslash
Loop{ each line' in vector of lines }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

				
\backslash
State $ 
\backslash
textit{ second line} 
\backslash
gets 
\backslash
emph{FulfillConstraints}(
\backslash
textit{first line, line'}) $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

			
\backslash
EndLoop
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		
\backslash
EndLoop
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

		
\backslash
If { there is( second line )}
\end_layout

\begin_layout Plain Layout

			
\end_layout

\begin_layout Plain Layout

			
\backslash
State $ 
\backslash
textit{ Tool Shaft} 
\backslash
gets 
\backslash
textit{(first line, second line)} $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

			
\backslash
Loop { each of shaft's lines }
\end_layout

\begin_layout Plain Layout

					
\end_layout

\begin_layout Plain Layout

					
\backslash
State $ 
\backslash
textit{vector of points} 
\backslash
gets 
\backslash
emph{ExtractEdgePoints}(
\backslash
textit{Tool Shaft}) $
\end_layout

\begin_layout Plain Layout

					
\end_layout

\begin_layout Plain Layout

					
\backslash
State $ 
\backslash
textit{ending points} 
\backslash
gets 
\backslash
emph{FindEndPoints}(
\backslash
textit{vector of points}) $
\end_layout

\begin_layout Plain Layout

					
\end_layout

\begin_layout Plain Layout

			
\backslash
EndLoop
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

			
\backslash
State $ 
\backslash
textit{TipEstimation} 
\backslash
gets 
\backslash
emph{CorrespondPoints}(
\backslash
textit{ ending points }) $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

			
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

			
\backslash
State $ 
\backslash
textit{Tip} 
\backslash
gets 
\backslash
emph{Arithimetic Mean}(
\backslash
textit{TipEstimation}) $
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	
\backslash
State End
\end_layout

\begin_layout Plain Layout


\backslash
EndProcedure
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
The presented solutions were developed in C++ language, based on OpenCV's
 libraries [X].
 The images were obtained from monocular recording of 
\emph on
in vivo
\emph default
 MIS from the web database 
\emph on

\begin_inset Quotes eld
\end_inset

Hamlyn Centre Laparoscopic / Endoscopic Video Datasets
\begin_inset Quotes erd
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout

\emph on
http://hamlyn.doc.ic.ac.uk/vision/
\end_layout

\end_inset


\emph default
 and from the Dr R.K.
 Mishra database
\begin_inset Foot
status open

\begin_layout Plain Layout
http://laptube.net/
\end_layout

\end_inset

.
 To assure the robustness of our solution, we tested it to a variety of
 images from different surgical operations.
 Each of them shall represent a different set of illumination and surgical
 tool's color 
\begin_inset ERT
status open

\begin_layout Plain Layout

---
\end_layout

\end_inset

 determinant characteristics for image for computer vision.
 In addition, 
\emph on
ex vivo
\emph default
 simulations were recorded in laboratory.
\end_layout

\begin_layout Standard
To evaluate the precision of our method, a set of 573 frames were analyzed,
 from five diferrent videos of our dataset.
 These frames were distributed in 304 high quality images and 269 low quality
 images.
 Images were extracted manually from each of the video files along with
 
\emph on
ground thruth(GT)
\emph default
 elaboration.
 The region considered as tip for 
\emph on
GT
\emph default
 consists in the entire metalic region in top of the instrument's body.
 Each of their results is expressed in Table 1.
\end_layout

\begin_layout Standard
Values considered for analysis were the absolute difference between the
 estimated tip position and the ideal
\emph on
 
\emph default
position (
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
mu$
\end_layout

\end_inset

), in pixels, their standard deviation (
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
sigma$
\end_layout

\end_inset

) and a 
\begin_inset Formula $hit$
\end_inset

 parameter.
 When intrument's Tip estimation is less than 20 pixels apart from the selected
 one, a 
\emph on

\begin_inset Formula $hit$
\end_inset

 
\emph default
is counted.
 It's valuable to notice that, when choosing the ideal position, the shortest
 distance between the real value and the metalic region of the tip in the
 image was considered.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
hit=\mu<20
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Regarding the measurement of the computational cost of the algorithm, we
 got the average value spent by all considered frames in the videos of out
 database, 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
eta$
\end_layout

\end_inset

.
 We got the following results represented by the table
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Regarding the measurement of the computational cost of the algorithm, we
 got the average frame rate value $
\backslash
eta$ spent by some considered frames in the videos of out database.
 The computer used for benchmarking was an an Intel® Core™ i3-2310M CPU
 @ 2.10GHz × 4 processor with 4 GB of RAM, which obtained an $
\backslash
eta$ of 24,62 fps.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This value is below the 26,98 fps obtained from Dokter [X], but in their
 measurements a powerfull computer was used.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Results in our dataset.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
mu$ (pixels)
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
sigma$ (pixels)
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

$hits$ ratio
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row topspace="0.2cm">
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
High Quality
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
15,535094145
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
22,6543975696
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
88,88%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Low Quality
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
17,8220273643
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
26,2574075607
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
86,62%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
The tool tracking algorithm has shown interesting results in the scope of
 single intrument's tip pose estimation.
 only if it first finds an instrument's body.
 Besides that, the scope of our work is limited for finding only black instrumen
ts shapes.
 Also, some instruments have written characters in it's body, which sometimes
 is confused with the instrument's tip.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The method presented here is one of the few that propose locating surgery
 tools in totally 2d, that is, using a single camera.
 By this, the computation time of our algorithm is very less compared with
 the methods using 3d vision, since all the calculations made are in only
 one image stream.
 In our scope, this is a strong requirement, as the robotic system will
 be used in precarious conditions of computing power.
 Besides that, in our scope the results obtained here was satisfactory.
\end_layout

\begin_layout Plain Layout
For future work, we think in replacing the Hough Lines method [X] by it's
 probabilistic version, which can give us the begining and ending points
 information of a line.
 This will help us to improve the finding for the instrument's tip, as like
 in Dokter algorithm[X].
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The method presented here is one of the few that propose locating surgery
 tools in totally 2d, that is, using a single camera.
 By this, the computation time of our algorithm is very less compared with
 the methods using 3d vision, since all the calculations made are in only
 one image stream.
 In our scope, this is a strong requirement, as the robotic system will
 be used in precarious conditions of computing power.
 Besides that, in our scope the results obtained here was satisfactory.
\end_layout

\begin_layout Standard
For future work, we think in replacing the default Hough Lines method by
 an extended fastest one, with the aid to decrease the overall computation
 time of our algorithm.
 We also think to use it's probabilist version to increase the finding tip
 accuracy.
 In expandind the scope of our method, we think in trying to integrate a
 better procedure to handle written characters instrument's bodies.
\end_layout

\begin_layout Section*
Acknowlegment
\end_layout

\begin_layout Standard
This project is funded by the Brazilian Ministry of Health (Termo de Cooperação
 nº 121/2013, Processo 25000.169843/2013-83).
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
